# 分词规则

## 查看分词结果

```json
GET /表名/_analyze
{
  "field": "字段名",
  "text": "Eating an apple a day & keeps the doctor away"
}
```

## 默认分词

![](https://cdn.jsdelivr.net/gh/kingmusi/blogImages/img/1613916568301.png)

## 英语分词

```json
PUT /movie
{
  "mappings": {
    "properties": {
      "name": {
        "type": "text",
        "analyzer": "english"
      }
    }
  }
}
```

![](https://cdn.jsdelivr.net/gh/kingmusi/blogImages/img/1613916953198.png)

## 中文分词

安装中文分词器（Ik插件）

1. 命令行安装

   ```shell
   cd bin
   elasticsearch-plugin install https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v7.11.1/elasticsearch-analysis-ik-7.11.1.zip
   ```

2. 在 [官网](https://github.com/medcl/elasticsearch-analysis-ik) 中的 release 中下载对应版本的压缩包

   到 elasticSearch 的 **plugins** 下解压

**2种分词规则**

1. **ik_smart**：尽最精简的分出词语。一般用于搜索关键词的分词
2. **ik_max_word**：只要匹配词库的，就选出。一般用于索引分词

> 例子：中华人民共和国国歌
>
> **ik_smart**：中华人民共和国、国歌
>
> **ik_max_word**：中华人民共和国、中华人民、中华、华人、人民共和国、人民、共和国、共和、国、国歌

![](https://cdn.jsdelivr.net/gh/kingmusi/blogImages/img/1614084935618.png)

> **词库扩展**
>
> - 在 `ik/config` 创建新的词库文件 `xx.dic`
>
> - 在 `xx.dic` 加入词，每个词一行
>
>   ```js
>   你好
>   你不好
>   ```
>
> - 把 `xx.dic` 用 **utf-8** 编码
>
> - 在 `IKAnalyzer.cfg.xml` 中加入词库文件
>
>   ```xml
>   <!--用户可以在这里配置自己的扩展字典 -->
>   <!--多个文件以;分隔 -->
>   <entry key="ext_dict">xxx1.dic;xxx2.dic</entry>
>   ```