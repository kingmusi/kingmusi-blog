# LangChain3 å¤šæ¨¡æ€èƒ½åŠ›æŒ‡å— ğŸ¨ğŸµğŸ¤

## ğŸ–¼ï¸ 1. å›¾ç‰‡è¯†åˆ«èƒ½åŠ›

### 1.1 åŸºç¡€å›¾ç‰‡è¯†åˆ«

```python
from langchain_core.messages import HumanMessage

# æ–¹æ³•1ï¼šä½¿ç”¨å›¾ç‰‡URL
image_url = "https://static.xiaohoucode.com/xh-python/a7f83564c2025e0096fdb8b22ff83d66/342111/53f28b7b-e009-47cd-b826-7c42ddb81fc4.jpg"

messages = HumanMessage(
    content=[
        {"type": "text", "text": "ç”¨ä¸­æ–‡è¯¦ç»†æè¿°ä¸€ä¸‹è¿™å¼ å›¾ç‰‡çš„å†…å®¹"},
        {"type": "image_url", "image_url": {"url": image_url}}
    ]
)

response = llm.invoke([messages])
print(response.content)
```

### 1.2 Base64ç¼–ç æ–¹å¼

> **æ³¨æ„**ï¼šå½“å›¾ç‰‡URLæ— æ³•ç›´æ¥è®¿é—®æ—¶ï¼Œå¯ä»¥ä½¿ç”¨Base64ç¼–ç æ–¹å¼ã€‚

```python
import base64
import httpx
from langchain_core.messages import HumanMessage

# æ–¹æ³•2ï¼šä½¿ç”¨Base64ç¼–ç ï¼ˆæ¨èï¼šå…¼å®¹æ€§æ›´å¥½ï¼‰
image_data = base64.b64encode(httpx.get(image_url).content).decode("utf-8")

messages = HumanMessage(
    content=[
        {"type": "text", "text": "åˆ†æè¿™å¼ å›¾ç‰‡ä¸­çš„ä¸»è¦å…ƒç´ "},
        {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{image_data}"}}
    ]
)

response = llm.invoke([messages])
print(response.content)
```

## ğŸµ 2. éŸ³é¢‘å¤„ç†èƒ½åŠ›

### 2.1 éŸ³é¢‘æ–‡ä»¶è¯†åˆ«

> **é‡ç‚¹**ï¼šéŸ³é¢‘å¤„ç†éœ€è¦æ¨¡å‹æ”¯æŒéŸ³é¢‘è¾“å…¥ï¼Œç›®å‰ä¸»è¦æ”¯æŒGPT-4Vç­‰é«˜çº§æ¨¡å‹ã€‚

```python
from langchain_core.messages import HumanMessage
import base64

def process_audio_file(audio_path: str, prompt: str = "è¯·æè¿°è¿™æ®µéŸ³é¢‘çš„å†…å®¹"):
    """å¤„ç†éŸ³é¢‘æ–‡ä»¶çš„é€šç”¨å‡½æ•°"""
    
    with open(audio_path, "rb") as audio_file:
        audio_data = base64.b64encode(audio_file.read()).decode("utf-8")
    
    messages = HumanMessage(
        content=[
            {"type": "text", "text": prompt},
            {"type": "audio_url", "audio_url": {"url": f"data:audio/wav;base64,{audio_data}"}}
        ]
    )
    
    return llm.invoke([messages])
```

### 2.2 éŸ³é¢‘URLå¤„ç†

```python
def process_audio_url(audio_url: str, prompt: str = "åˆ†æè¿™æ®µéŸ³é¢‘"):
    """å¤„ç†éŸ³é¢‘URLçš„å‡½æ•°"""
    
    messages = HumanMessage(
        content=[
            {"type": "text", "text": prompt},
            {"type": "audio_url", "audio_url": {"url": audio_url}}
        ]
    )
    
    return llm.invoke([messages])
```

## ğŸ¤ 3. è¯­éŸ³è½¬æ–‡æœ¬èƒ½åŠ›

### 3.1 åŸºç¡€è¯­éŸ³è¯†åˆ«

```python
import openai

openai.api_key = "xxx"

with open("assets/test_audio.wav", "rb") as audio_file:
    response = openai.audio.transcriptions.create(
        model="whisper-1",
        file=audio_file,
        response_format="text",
    )
```

## ğŸ”§ 4. å¤šæ¨¡æ€æ¶ˆæ¯æ ¼å¼è¯¦è§£

### 4.1 Message Content ç»“æ„

**LangChain3çš„å¤šæ¨¡æ€æ¶ˆæ¯é‡‡ç”¨æ ‡å‡†åŒ–çš„contentæ•°ç»„æ ¼å¼**ï¼š

```python
messages = HumanMessage(
    content=[
        # æ–‡æœ¬å†…å®¹
        {"type": "text", "text": "è¯·åˆ†æä»¥ä¸‹å†…å®¹"},
        
        # å›¾ç‰‡å†…å®¹
        {"type": "image_url", "image_url": {"url": "https://example.com/image.jpg"}},
    ]
)
```

### 4.2 æ”¯æŒçš„æ•°æ®ç±»å‹

| ç±»å‹ | è¯´æ˜ | ç¤ºä¾‹ |
|------|------|------|
| `text` | æ–‡æœ¬å†…å®¹ | `{"type": "text", "text": "æè¿°"}` |
| `image_url` | å›¾ç‰‡URLæˆ–Base64 | `{"type": "image_url", "image_url": {"url": "..."}}` |
