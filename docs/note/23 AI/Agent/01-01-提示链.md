# æç¤ºé“¾

```markmap
# æç¤ºé“¾
## æ ¸å¿ƒæ¦‚å¿µ
- å®šä¹‰ä¸æœ¬è´¨
- ä¼˜ç¼ºç‚¹åˆ†æ
## è®¾è®¡æ€ç»´
- åˆ†è€Œæ²»ä¹‹ç­–ç•¥
- ç»“æ„åŒ–è¾“å‡º
- è§’è‰²åˆ†é…
## åº”ç”¨åœºæ™¯
- ä¿¡æ¯å¤„ç†æµç¨‹
- å¤æ‚é—®ç­”
- æ•°æ®æå–ä¸è½¬æ¢
- å†…å®¹ç”Ÿæˆæµç¨‹
- æœ‰çŠ¶æ€å¯¹è¯æ™ºèƒ½ä½“
- ä»£ç ç”Ÿæˆä¸ä¼˜åŒ–
```

## æç¤ºé“¾è®¾è®¡æ€ç»´

**æç¤ºé“¾ï¼ˆPrompt Chainingï¼‰**ï¼Œä¹Ÿç§°ä¸º**æµæ°´çº¿ï¼ˆPipelineï¼‰æ¨¡å¼**ï¼Œæ˜¯å°†å¤æ‚ä»»åŠ¡æ‹†è§£ä¸ºä¸€ç³»åˆ—æ›´å°ã€æ›´æ˜“ç®¡ç†çš„å­é—®é¢˜ï¼Œ**æ¯ä¸ªå­é—®é¢˜é€šè¿‡ä¸“é—¨è®¾è®¡çš„æç¤ºå•ç‹¬å¤„ç†ï¼Œå¹¶å°†å‰ä¸€æ­¥çš„è¾“å‡ºä½œä¸ºä¸‹ä¸€æ­¥çš„è¾“å…¥ï¼Œå½¢æˆé“¾å¼ä¾èµ–**ã€‚

> **æ ¸å¿ƒæ€æƒ³**ï¼šä¸å…¶è®© LLM ä¸€æ­¥åˆ°ä½è§£å†³å¤æ‚é—®é¢˜ï¼Œæç¤ºé“¾é‡‡ç”¨åˆ†è€Œæ²»ä¹‹ç­–ç•¥ï¼Œ**æ¯ä¸€æ­¥çš„è¾“å‡ºä½œä¸ºä¸‹ä¸€æ­¥çš„è¾“å…¥è‡³å…³é‡è¦**ï¼Œè¿™ç§ä¿¡æ¯ä¼ é€’å»ºç«‹äº†ä¾èµ–é“¾ï¼Œä½¿ LLM èƒ½å¤Ÿåœ¨å‰ä¸€æ­¥åŸºç¡€ä¸Šä¸æ–­å®Œå–„ç†è§£ï¼Œé€æ­¥é€¼è¿‘ç›®æ ‡è§£ã€‚

1. **æ˜ç¡®æ¯ä¸ªæ­¥éª¤çš„ç›®æ ‡**ï¼šæ¯ä¸ªæç¤ºéƒ½åº”è¯¥æœ‰æ¸…æ™°ã€å•ä¸€çš„ç›®æ ‡ï¼Œé¿å…åœ¨ä¸€ä¸ªæç¤ºä¸­å¤„ç†å¤šä¸ªä¸ç›¸å…³çš„ä»»åŠ¡
2. **åˆç†è®¾è®¡ä¸Šä¸‹æ–‡ä¼ é€’**ï¼šå‰ä¸€ä¸ªæ­¥éª¤çš„è¾“å‡ºåº”è¯¥åŒ…å«ä¸‹ä¸€ä¸ªæ­¥éª¤æ‰€éœ€çš„å…³é”®ä¿¡æ¯ï¼Œä½†ä¹Ÿè¦é¿å…ä¼ é€’è¿‡å¤šå†—ä½™ä¿¡æ¯
3. **ä½¿ç”¨è§’è‰²å®šä½**ï¼šä¸ºæ¯ä¸ªæ­¥éª¤åˆ†é…æ˜ç¡®çš„è§’è‰²ï¼Œå¦‚"å¸‚åœºåˆ†æå¸ˆ"ã€"æŠ€æœ¯ä¸“å®¶"ç­‰ï¼Œæå‡ä¸“ä¸šæ€§
4. **ç»“æ„åŒ–è¾“å‡º**ï¼šæå‡ä¸‹ä¸€æ­¥æç¤ºè¯æ‹¼æ¥çš„çµæ´»æ€§

## ä¸ºä»€ä¹ˆéœ€è¦æç¤ºé“¾ï¼Ÿ

#### å•ä¸€æç¤ºçš„å±€é™æ€§ âš ï¸

å¯¹äºå¤šå±‚æ¬¡ä»»åŠ¡ï¼Œ**å•ä¸€å¤æ‚æç¤ºå¾€å¾€æ•ˆç‡ä½ä¸‹**ï¼Œæ¨¡å‹å®¹æ˜“å‡ºç°ä»¥ä¸‹é—®é¢˜ï¼š

- **å¿½ç•¥éƒ¨åˆ†æŒ‡ä»¤**ï¼šæ¨¡å‹å¯èƒ½åªå®Œæˆéƒ¨åˆ†ä»»åŠ¡ï¼Œé—æ¼å…³é”®ç¯èŠ‚
- **ä¸¢å¤±ä¸Šä¸‹æ–‡**ï¼šåœ¨é•¿æ–‡æœ¬å¤„ç†ä¸­ï¼Œæ¨¡å‹å¯èƒ½æ— æ³•ä¿æŒå®Œæ•´çš„ä¸Šä¸‹æ–‡ç†è§£
- **é”™è¯¯ç´¯ç§¯**ï¼šä¸€æ­¥åˆ°ä½çš„å¤„ç†æ–¹å¼ï¼Œé”™è¯¯ä¼šç›´æ¥ä½“ç°åœ¨æœ€ç»ˆè¾“å‡ºä¸­
- **ä¸Šä¸‹æ–‡çª—å£ä¸è¶³**ï¼šå¤æ‚ä»»åŠ¡éœ€è¦å¤§é‡ä¸Šä¸‹æ–‡ï¼Œå¯èƒ½è¶…å‡ºæ¨¡å‹çš„çª—å£é™åˆ¶
- **å‡ºç°å¹»è§‰**ï¼šæ¨¡å‹å¯èƒ½ç”Ÿæˆçœ‹ä¼¼åˆç†ä½†å®é™…é”™è¯¯çš„ä¿¡æ¯

> **ç¤ºä¾‹**ï¼šè¦æ±‚æ¨¡å‹åˆ†æå¸‚åœºè°ƒç ”æŠ¥å‘Šã€æ€»ç»“å‘ç°ã€æå–æ•°æ®ç‚¹å¹¶æ’°å†™é‚®ä»¶ï¼Œå•ä¸€æç¤ºå¯èƒ½å¯¼è‡´æ¨¡å‹åªå®Œæˆéƒ¨åˆ†ä»»åŠ¡ï¼Œé—æ¼å…³é”®ç¯èŠ‚ã€‚

#### æç¤ºé“¾çš„ä¼˜åŠ¿ âœ…

**æç¤ºé“¾é€šè¿‡å°†å¤æ‚ä»»åŠ¡æ‹†è§£ä¸ºèšç„¦çš„é¡ºåºæµç¨‹ï¼Œæ˜¾è‘—æå‡å¯é æ€§å’Œå¯æ§æ€§**ï¼š

1. **æ¨¡å—åŒ–è®¾è®¡**ï¼šå°†å¤æ‚ä»»åŠ¡æ‹†åˆ†ä¸ºç‹¬ç«‹çš„æ¨¡å—ï¼Œæ¯ä¸ªæ¨¡å—è´Ÿè´£ä¸€ä¸ªç‰¹å®šåŠŸèƒ½ï¼Œæ›´å®¹æ˜“ç†è§£ã€è°ƒè¯•å’Œç»´æŠ¤
2. **æé«˜å‡†ç¡®æ€§**ï¼šæ¯ä¸ªæ­¥éª¤çš„è¾“å‡ºéƒ½ç»è¿‡ç²¾å¿ƒè®¾è®¡çš„æç¤ºè¿›è¡Œçº¦æŸå’Œä¼˜åŒ–ï¼Œå‡å°‘é”™è¯¯ä¼ æ’­çš„å¯èƒ½æ€§
3. **å¢å¼ºå¯è§£é‡Šæ€§**ï¼šæ¯ä¸ªæ­¥éª¤éƒ½ä¼šäº§ç”Ÿä¸­é—´è¾“å‡ºï¼Œè¿™äº›ä¸­é—´ç»“æœå¯ä»¥è¢«æ£€æŸ¥ã€éªŒè¯å’Œè°ƒè¯•
4. **çµæ´»ç»„åˆ**ï¼šå¯ä»¥æ ¹æ®éœ€æ±‚åŠ¨æ€è°ƒæ•´æ­¥éª¤ï¼Œæ·»åŠ ã€åˆ é™¤æˆ–é‡æ–°æ’åº

> **è§’è‰²åˆ†é…**ï¼šä¸ºç¡®ä¿æ¯æ­¥ä»»åŠ¡å‡†ç¡®ï¼Œå¯ä¸ºæ¨¡å‹åˆ†é…ä¸åŒè§’è‰²ï¼Œå¦‚"å¸‚åœºåˆ†æå¸ˆ"ã€"è´¸æ˜“åˆ†æå¸ˆ"ã€"æ–‡æ¡£ä¸“å®¶"ç­‰ï¼Œè®©æ¯ä¸ªæ­¥éª¤éƒ½æœ‰æ˜ç¡®çš„ä¸“ä¸šå®šä½ã€‚

## åº”ç”¨åœºæ™¯ä¸æµç¨‹æ„å»º

### 1. ä¿¡æ¯å¤„ç†æµç¨‹ ğŸ“„

**åœºæ™¯**ï¼šè®¸å¤šä»»åŠ¡éœ€å¯¹åŸå§‹ä¿¡æ¯å¤šæ¬¡è½¬æ¢ï¼Œå¦‚æ–‡æ¡£æ‘˜è¦ã€å®ä½“æå–ã€ç”¨å®ä½“æŸ¥è¯¢æ•°æ®åº“ã€ç”ŸæˆæŠ¥å‘Š

**æç¤ºé“¾**ï¼š

1. ä»æŒ‡å®šURLæˆ–æ–‡æ¡£æå–æ–‡æœ¬å†…å®¹
2. æ‘˜è¦æ¸…æ´—åçš„æ–‡æœ¬
3. ä»æ‘˜è¦æˆ–åŸæ–‡ä¸­æå–å®ä½“ï¼ˆå¦‚å§“åã€æ—¥æœŸã€åœ°ç‚¹ï¼‰
4. ç”¨å®ä½“æŸ¥è¯¢å†…éƒ¨çŸ¥è¯†åº“
5. ç”ŸæˆåŒ…å«æ‘˜è¦ã€å®ä½“å’ŒæŸ¥è¯¢ç»“æœçš„æœ€ç»ˆæŠ¥å‘Š

**æµç¨‹æ„å»º**ï¼š

```python
from typing import TypedDict, List
from pydantic import BaseModel
from langgraph.graph import StateGraph, END
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser, PydanticOutputParser
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(temperature=0)

class Entity(BaseModel):
    name: str
    date: str
    location: str
    organization: str

class AgentState(TypedDict):
    source: str
    text: str
    summary: str
    entities: List[Entity]
    query_results: str
    final_report: str

# æç¤º 1ï¼šæå–æ–‡æœ¬å†…å®¹
def extract_node(state: AgentState):
    prompt = ChatPromptTemplate.from_template("""ä½ æ˜¯æ–‡æ¡£å¤„ç†ä¸“å®¶ã€‚è¯·ä»ä»¥ä¸‹ URL æˆ–æ–‡æ¡£ä¸­æå–æ–‡æœ¬å†…å®¹ï¼š

{source}

è¾“å‡ºæ ¼å¼ï¼šçº¯æ–‡æœ¬å†…å®¹ï¼Œå»é™¤æ ¼å¼æ ‡è®°ã€‚""")
    chain = prompt | llm | StrOutputParser()
    return {"text": chain.invoke({"source": state["source"]})}

# æç¤º 2ï¼šæ‘˜è¦æ–‡æœ¬
def summarize_node(state: AgentState):
    prompt = ChatPromptTemplate.from_template("""ä½ æ˜¯å†…å®¹æ‘˜è¦ä¸“å®¶ã€‚è¯·å¯¹ä»¥ä¸‹æ–‡æœ¬è¿›è¡Œæ‘˜è¦ï¼Œä¿ç•™å…³é”®ä¿¡æ¯ï¼š
    
{text}

è¾“å‡ºæ ¼å¼ï¼š200å­—ä»¥å†…çš„æ‘˜è¦ã€‚""")
    chain = prompt | llm | StrOutputParser()
    return {"summary": chain.invoke({"text": state["text"]})}

# æç¤º 3ï¼šæå–å®ä½“
def extract_entities_node(state: AgentState):
    summary = state.get('summary', '')
    parser = PydanticOutputParser(pydantic_object=Entity)
    format_instructions = parser.get_format_instructions()
    prompt = ChatPromptTemplate.from_template("""ä½ æ˜¯ä¿¡æ¯æå–ä¸“å®¶ã€‚è¯·ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æå–å®ä½“ä¿¡æ¯ï¼š
    
{summary}

éœ€è¦æå–çš„å®ä½“ç±»å‹ï¼š
- äººå
- æ—¥æœŸ
- åœ°ç‚¹
- ç»„ç»‡

{format_instructions}""")
    chain = prompt | llm | parser
    entity = chain.invoke({"summary": state["summary"], "format_instructions": format_instructions})
    return {"entities": [entity]}

# æç¤º 4ï¼šæŸ¥è¯¢çŸ¥è¯†åº“
def query_kb_node(state: AgentState):
    entities = state.get('entities', [])
    entities_str = "\n".join([f"- {entity.name} ({entity.date}, {entity.location}, {entity.organization})" for entity in entities])
    prompt = ChatPromptTemplate.from_template("""ä½ æ˜¯çŸ¥è¯†åº“æŸ¥è¯¢ä¸“å®¶ã€‚æ ¹æ®ä»¥ä¸‹å®ä½“ä¿¡æ¯ï¼ŒæŸ¥è¯¢ç›¸å…³çŸ¥è¯†ï¼š

{entities_str}

è¾“å‡ºæ ¼å¼ï¼š200å­—ä»¥å†…çš„æŸ¥è¯¢ç»“æœï¼Œä¸è¦æœ‰ä»»ä½•è§£é‡Šã€‚""")
    chain = prompt | llm | StrOutputParser()
    return {"query_results": chain.invoke({"entities_str": entities_str})}

# æç¤º 5ï¼šç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
def report_node(state: AgentState):
    summary = state.get('summary', '')
    entities = state.get('entities', [])
    entities_str = "\n".join([f"- {entity.name} ({entity.date}, {entity.location}, {entity.organization})" for entity in entities])
    query_results = state.get('query_results', '')
    prompt = ChatPromptTemplate.from_template("""ä½ æ˜¯æŠ¥å‘Šæ’°å†™ä¸“å®¶ã€‚è¯·åŸºäºä»¥ä¸‹ä¿¡æ¯ç”Ÿæˆç»¼åˆæŠ¥å‘Šï¼š
    
æ‘˜è¦ï¼š{summary}
å®ä½“ï¼š{entities_str}
æŸ¥è¯¢ç»“æœï¼š{query_results}

è¾“å‡ºæ ¼å¼ï¼šç»“æ„åŒ–çš„ Markdown æŠ¥å‘Šï¼ŒåŒ…å«æ ‡é¢˜ã€æ‘˜è¦ã€å…³é”®å‘ç°ã€ç»“è®ºç­‰éƒ¨åˆ†ã€‚""")
    chain = prompt | llm | StrOutputParser()
    return {"final_report": chain.invoke({"summary": summary, "entities_str": entities_str, "query_results": query_results})}

# æ„å»ºå›¾
workflow = StateGraph(AgentState)

# æ·»åŠ èŠ‚ç‚¹
workflow.add_node("extract", extract_node)
workflow.add_node("summarize", summarize_node)
workflow.add_node("entities", extract_entities_node)
workflow.add_node("query", query_kb_node)
workflow.add_node("report", report_node)

# å®šä¹‰è¿çº¿
workflow.set_entry_point("extract")
workflow.add_edge("extract", "summarize")
workflow.add_edge("summarize", "entities")
workflow.add_edge("entities", "query")
workflow.add_edge("query", "report")
workflow.add_edge("report", END)

# ç¼–è¯‘
app = workflow.compile()

# è¿è¡Œ
inputs = {"source": "http://example.com/data"}
result = app.invoke(inputs)
print(result["final_report"])
```

### 2. å¤æ‚é—®ç­” ğŸ’¬

**åœºæ™¯**ï¼šå›ç­”éœ€è¦å¤šæ­¥æ¨ç†æˆ–ä¿¡æ¯æ£€ç´¢çš„é—®é¢˜

**æç¤ºé“¾**ï¼š

1. è¯†åˆ«ç”¨æˆ·é—®é¢˜çš„æ ¸å¿ƒå­é—®é¢˜ï¼ˆå´©ç›˜åŸå› ã€æ”¿åºœåº”å¯¹ï¼‰
2. æ£€ç´¢ç¬¬ä¸€ä¸ªå­é—®é¢˜çš„ç›¸å…³ä¿¡æ¯
3. æ£€ç´¢ç¬¬äºŒä¸ªå­é—®é¢˜çš„ç›¸å…³ä¿¡æ¯
4. ç»¼åˆæ‰€æœ‰ä¿¡æ¯ï¼Œå½¢æˆå®Œæ•´ç­”æ¡ˆ

**æµç¨‹æ„å»º**ï¼š

```python
from typing import TypedDict, List
from pydantic import BaseModel
from langgraph.graph import StateGraph, END
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser, JsonOutputParser
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(temperature=0)

class SubQuestion(BaseModel):
    question: str
    type: str  # åŸå› /åº”å¯¹/å½±å“ç­‰

class AgentState(TypedDict):
    question: str
    sub_questions: List[SubQuestion]
    answer_1: str
    answer_2: str
    final_answer: str

# æç¤º 1ï¼šè¯†åˆ«å­é—®é¢˜
def decompose_node(state: AgentState):
    prompt = ChatPromptTemplate.from_template("""ä½ æ˜¯é—®é¢˜åˆ†æä¸“å®¶ã€‚è¯·åˆ†æä»¥ä¸‹é—®é¢˜ï¼Œè¯†åˆ«éœ€è¦å›ç­”çš„æ ¸å¿ƒå­é—®é¢˜ï¼š

é—®é¢˜ï¼š{question}

è¾“å‡ºæ ¼å¼ï¼šJSON æ ¼å¼ï¼ŒåŒ…å« sub_questions æ•°ç»„ï¼Œæ¯ä¸ªå­é—®é¢˜åŒ…å« question å’Œ typeï¼ˆåŸå› /åº”å¯¹/å½±å“ç­‰ï¼‰ã€‚""")
    parser = JsonOutputParser(pydantic_object=List[SubQuestion])
    chain = prompt | llm | parser
    result = chain.invoke({"question": state["question"]})
    return {"sub_questions": result}

# æç¤º 2ï¼šæ£€ç´¢ç¬¬ä¸€ä¸ªå­é—®é¢˜
def retrieve_1_node(state: AgentState):
    sub_questions = state.get("sub_questions", [])
    if not sub_questions:
        return {"answer_1": ""}
    sub_q = sub_questions[0]
    prompt = ChatPromptTemplate.from_template("""ä½ æ˜¯ä¿¡æ¯æ£€ç´¢ä¸“å®¶ã€‚è¯·æ£€ç´¢å…³äºä»¥ä¸‹é—®é¢˜çš„ç›¸å…³ä¿¡æ¯ï¼š

å­é—®é¢˜ï¼š{sub_question}

è¾“å‡ºæ ¼å¼ï¼šè¯¦ç»†çš„ä¿¡æ¯æ‘˜è¦ï¼ŒåŒ…å«å…³é”®äº‹å®å’Œæ•°æ®ã€‚""")
    chain = prompt | llm | StrOutputParser()
    return {"answer_1": chain.invoke({"sub_question": sub_q.question})}

# æç¤º 3ï¼šæ£€ç´¢ç¬¬äºŒä¸ªå­é—®é¢˜
def retrieve_2_node(state: AgentState):
    sub_questions = state.get("sub_questions", [])
    if len(sub_questions) < 2:
        return {"answer_2": ""}
    sub_q = sub_questions[1]
    prompt = ChatPromptTemplate.from_template("""ä½ æ˜¯ä¿¡æ¯æ£€ç´¢ä¸“å®¶ã€‚è¯·æ£€ç´¢å…³äºä»¥ä¸‹é—®é¢˜çš„ç›¸å…³ä¿¡æ¯ï¼š

å­é—®é¢˜ï¼š{sub_question}

è¾“å‡ºæ ¼å¼ï¼šè¯¦ç»†çš„ä¿¡æ¯æ‘˜è¦ï¼ŒåŒ…å«å…³é”®äº‹å®å’Œæ•°æ®ã€‚""")
    chain = prompt | llm | StrOutputParser()
    return {"answer_2": chain.invoke({"sub_question": sub_q.question})}

# æç¤º 4ï¼šç»¼åˆç­”æ¡ˆ
def synthesize_node(state: AgentState):
    prompt = ChatPromptTemplate.from_template("""ä½ æ˜¯ç­”æ¡ˆç»¼åˆä¸“å®¶ã€‚è¯·åŸºäºä»¥ä¸‹ä¿¡æ¯ï¼Œå½¢æˆå®Œæ•´çš„ç­”æ¡ˆï¼š

åŸé—®é¢˜ï¼š{question}
å­é—®é¢˜1çš„ç­”æ¡ˆï¼š{answer_1}
å­é—®é¢˜2çš„ç­”æ¡ˆï¼š{answer_2}

è¾“å‡ºæ ¼å¼ï¼šç»“æ„åŒ–çš„ç­”æ¡ˆï¼ŒåŒ…å«ï¼š
1. é—®é¢˜æ¦‚è¿°
2. å„å­é—®é¢˜çš„è¯¦ç»†å›ç­”
3. ç»¼åˆç»“è®º""")
    chain = prompt | llm | StrOutputParser()
    return {"final_answer": chain.invoke({
        "question": state["question"],
        "answer_1": state.get("answer_1", ""),
        "answer_2": state.get("answer_2", "")
    })}

# æ„å»ºå›¾
workflow = StateGraph(AgentState)

# æ·»åŠ èŠ‚ç‚¹
workflow.add_node("decompose", decompose_node)
workflow.add_node("retrieve_1", retrieve_1_node)
workflow.add_node("retrieve_2", retrieve_2_node)
workflow.add_node("synthesize", synthesize_node)

# å®šä¹‰è¿çº¿
workflow.set_entry_point("decompose")
workflow.add_edge("decompose", "retrieve_1")
workflow.add_edge("retrieve_1", "retrieve_2")
workflow.add_edge("retrieve_2", "synthesize")
workflow.add_edge("synthesize", END)

# ç¼–è¯‘
app = workflow.compile()

# è¿è¡Œ
inputs = {"question": "1929 å¹´è‚¡å¸‚å´©ç›˜çš„ä¸»è¦åŸå› åŠæ”¿åºœæ”¿ç­–åº”å¯¹ï¼Ÿ"}
result = app.invoke(inputs)
print(result["final_answer"])
```

### 3. æ•°æ®æå–ä¸è½¬æ¢ ğŸ”„

**åœºæ™¯**ï¼šä»éç»“æ„åŒ–æ–‡æœ¬ï¼ˆå‘ç¥¨ã€è¡¨å•ã€é‚®ä»¶ï¼‰ä¸­æå–ç»“æ„åŒ–æ•°æ®

**æç¤ºé“¾**ï¼š

1. å°è¯•ä»æ–‡æ¡£ä¸­æå–æŒ‡å®šå­—æ®µï¼ˆå§“åã€åœ°å€ã€é‡‘é¢ã€æ—¥æœŸï¼‰
2. æ£€æŸ¥å­—æ®µæ˜¯å¦é½å…¨ä¸”æ ¼å¼æ­£ç¡®
3. è‹¥å­—æ®µç¼ºå¤±æˆ–æ ¼å¼é”™è¯¯ï¼Œé‡æ–°æç¤ºæ¨¡å‹æŸ¥æ‰¾ç¼ºå¤±/é”™è¯¯ä¿¡æ¯
4. å†æ¬¡éªŒè¯ç»“æœï¼Œå¿…è¦æ—¶é‡å¤æ­¥éª¤3
5. æ ¼å¼åŒ–è¾“å‡ºä¸ºæ ‡å‡†æ ¼å¼

**æµç¨‹æ„å»º**ï¼š

```python
from typing import TypedDict, Optional
from pydantic import BaseModel
from langgraph.graph import StateGraph, END
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser, JsonOutputParser
from langchain_openai import ChatOpenAI
import json

llm = ChatOpenAI(temperature=0)

class ExtractedData(BaseModel):
    name: Optional[str] = None
    address: Optional[str] = None
    amount: Optional[str] = None
    date: Optional[str] = None

class AgentState(TypedDict):
    document: str
    extracted_data: str
    validation_count: int
    validated_data: str
    formatted_data: str

# æç¤º 1ï¼šåˆæ­¥æå–
def extract_node(state: AgentState):
    prompt = ChatPromptTemplate.from_template("""ä½ æ˜¯æ•°æ®æå–ä¸“å®¶ã€‚è¯·ä»ä»¥ä¸‹æ–‡æ¡£ä¸­æå–æŒ‡å®šå­—æ®µï¼š

æ–‡æ¡£å†…å®¹ï¼š{document}

éœ€è¦æå–çš„å­—æ®µï¼š
- å§“å
- åœ°å€
- é‡‘é¢
- æ—¥æœŸ

è¾“å‡ºæ ¼å¼ï¼šJSON æ ¼å¼ï¼ŒåŒ…å«æ‰€æœ‰å­—æ®µï¼Œå¦‚æœæŸä¸ªå­—æ®µæ— æ³•æå–ï¼Œä½¿ç”¨ nullã€‚""")
    parser = JsonOutputParser()
    chain = prompt | llm | parser
    result = chain.invoke({"document": state["document"]})
    return {"extracted_data": json.dumps(result, ensure_ascii=False), "validation_count": 0}

# æç¤º 2ï¼šéªŒè¯å’Œè¡¥å…¨
def validate_node(state: AgentState):
    extracted_data = state.get("extracted_data", "{}")
    validation_count = state.get("validation_count", 0)
    
    prompt = ChatPromptTemplate.from_template("""ä½ æ˜¯æ•°æ®éªŒè¯ä¸“å®¶ã€‚è¯·æ£€æŸ¥ä»¥ä¸‹æå–ç»“æœï¼ŒæŸ¥æ‰¾ç¼ºå¤±æˆ–é”™è¯¯çš„å­—æ®µï¼š

æå–ç»“æœï¼š{extracted_data}
åŸå§‹æ–‡æ¡£ï¼š{document}

å¦‚æœå­—æ®µç¼ºå¤±æˆ–æ ¼å¼é”™è¯¯ï¼Œè¯·é‡æ–°æŸ¥æ‰¾å¹¶ä¿®æ­£ã€‚

è¾“å‡ºæ ¼å¼ï¼šJSON æ ¼å¼ï¼ŒåŒ…å«æ‰€æœ‰å­—æ®µçš„å®Œæ•´å’Œæ­£ç¡®å€¼ã€‚""")
    parser = JsonOutputParser()
    chain = prompt | llm | parser
    result = chain.invoke({
        "extracted_data": extracted_data,
        "document": state["document"]
    })
    
    # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰å­—æ®µéƒ½æœ‰å€¼
    all_fields_present = all([
        result.get("name"),
        result.get("address"),
        result.get("amount"),
        result.get("date")
    ])
    
    return {
        "validated_data": json.dumps(result, ensure_ascii=False),
        "validation_count": validation_count + 1
    }

# æ¡ä»¶åˆ¤æ–­ï¼šæ˜¯å¦éœ€è¦é‡æ–°éªŒè¯
def should_revalidate(state: AgentState) -> str:
    validated_data = json.loads(state.get("validated_data", "{}"))
    validation_count = state.get("validation_count", 0)
    
    # æ£€æŸ¥å­—æ®µå®Œæ•´æ€§
    all_fields_present = all([
        validated_data.get("name"),
        validated_data.get("address"),
        validated_data.get("amount"),
        validated_data.get("date")
    ])
    
    # å¦‚æœå­—æ®µä¸å®Œæ•´ä¸”æœªè¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œç»§ç»­éªŒè¯
    if not all_fields_present and validation_count < 3:
        return "validate"
    else:
        return "format"

# æç¤º 3ï¼šæ ¼å¼åŒ–è¾“å‡º
def format_node(state: AgentState):
    validated_data = json.loads(state.get("validated_data", "{}"))
    prompt = ChatPromptTemplate.from_template("""ä½ æ˜¯æ•°æ®æ ¼å¼åŒ–ä¸“å®¶ã€‚è¯·å°†ä»¥ä¸‹æ•°æ®è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼ï¼š

æ•°æ®ï¼š{validated_data}

è¦æ±‚ï¼š
- é‡‘é¢ç»Ÿä¸€ä¸ºæ•°å­—æ ¼å¼ï¼ˆå¦‚ï¼š1050.00ï¼‰
- æ—¥æœŸç»Ÿä¸€ä¸º YYYY-MM-DD æ ¼å¼
- åœ°å€è§„èŒƒåŒ–

è¾“å‡ºæ ¼å¼ï¼šJSON æ ¼å¼ï¼ŒåŒ…å«æ ¼å¼åŒ–åçš„æ‰€æœ‰å­—æ®µã€‚""")
    parser = JsonOutputParser()
    chain = prompt | llm | parser
    result = chain.invoke({"validated_data": json.dumps(validated_data, ensure_ascii=False)})
    return {"formatted_data": json.dumps(result, ensure_ascii=False)}

# æ„å»ºå›¾
workflow = StateGraph(AgentState)

# æ·»åŠ èŠ‚ç‚¹
workflow.add_node("extract", extract_node)
workflow.add_node("validate", validate_node)
workflow.add_node("format", format_node)

# å®šä¹‰è¿çº¿
workflow.set_entry_point("extract")
workflow.add_edge("extract", "validate")
workflow.add_conditional_edges(
    "validate",
    should_revalidate,
    {
        "validate": "validate",  # ç»§ç»­éªŒè¯
        "format": "format"       # æ ¼å¼åŒ–
    }
)
workflow.add_edge("format", END)

# ç¼–è¯‘
app = workflow.compile()

# è¿è¡Œ
inputs = {"document": "å‘ç¥¨å†…å®¹ï¼šå¼ ä¸‰ï¼ŒåŒ—äº¬å¸‚æœé˜³åŒºï¼Œå£¹ä»Ÿé›¶äº”åå…ƒï¼Œ2024å¹´1æœˆ15æ—¥"}
result = app.invoke(inputs)
print(result["formatted_data"])
```

### 4. å†…å®¹ç”Ÿæˆæµç¨‹ ğŸ“

**åœºæ™¯**ï¼šè‡ªåŠ¨åŒ–åˆ›æ„å†™ä½œã€æŠ€æœ¯æ–‡æ¡£ç”Ÿæˆ

**æç¤ºé“¾**ï¼š

1. æ ¹æ®ç”¨æˆ·å…´è¶£ç”Ÿæˆ 5 ä¸ªä¸»é¢˜åˆ›æ„
2. ç”¨æˆ·é€‰æ‹©æˆ–è‡ªåŠ¨é€‰å®šä¸€ä¸ªä¸»é¢˜
3. åŸºäºé€‰å®šä¸»é¢˜ç”Ÿæˆè¯¦ç»†å¤§çº²
4. æ ¹æ®å¤§çº²ç¬¬ä¸€ç‚¹æ’°å†™è‰ç¨¿
5. æ ¹æ®ç¬¬äºŒç‚¹æ’°å†™è‰ç¨¿ï¼Œå¹¶æä¾›å‰ä¸€æ®µä¸Šä¸‹æ–‡ï¼Œä¾æ¬¡å®Œæˆæ‰€æœ‰å¤§çº²ç‚¹
6. æ•´ä½“å®¡é˜…å¹¶ä¼˜åŒ–è‰ç¨¿çš„è¿è´¯æ€§ã€è¯­æ°”å’Œè¯­æ³•

**æµç¨‹æ„å»º**ï¼š

```python
from typing import TypedDict, List, Dict
from pydantic import BaseModel
from langgraph.graph import StateGraph, END
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser, JsonOutputParser
from langchain_openai import ChatOpenAI
import json

llm = ChatOpenAI(temperature=0)

class Topic(BaseModel):
    title: str
    brief_description: str

class OutlineItem(BaseModel):
    section_title: str
    key_points: List[str]

class AgentState(TypedDict):
    user_interests: str
    content_type: str
    topics: List[Dict]
    selected_topic: str
    outline: List[Dict]
    sections: List[str]
    current_section_index: int
    full_draft: str
    optimized_draft: str

# æç¤º 1ï¼šç”Ÿæˆä¸»é¢˜åˆ›æ„
def generate_topics_node(state: AgentState):
    prompt = ChatPromptTemplate.from_template("""ä½ æ˜¯åˆ›æ„ç­–åˆ’ä¸“å®¶ã€‚æ ¹æ®ç”¨æˆ·çš„å…´è¶£å’Œéœ€æ±‚ï¼Œç”Ÿæˆ 5 ä¸ªä¸»é¢˜åˆ›æ„ï¼š

ç”¨æˆ·å…´è¶£ï¼š{user_interests}
å†…å®¹ç±»å‹ï¼š{content_type}

è¾“å‡ºæ ¼å¼ï¼šJSON æ ¼å¼ï¼ŒåŒ…å« topics æ•°ç»„ï¼Œæ¯ä¸ªä¸»é¢˜åŒ…å« title å’Œ brief_descriptionã€‚""")
    parser = JsonOutputParser()
    chain = prompt | llm | parser
    result = chain.invoke({
        "user_interests": state["user_interests"],
        "content_type": state["content_type"]
    })
    return {"topics": result.get("topics", [])}

# æç¤º 2ï¼šé€‰æ‹©ä¸»é¢˜ï¼ˆè¿™é‡Œç®€åŒ–ä¸ºè‡ªåŠ¨é€‰æ‹©ç¬¬ä¸€ä¸ªï¼‰
def select_topic_node(state: AgentState):
    topics = state.get("topics", [])
    if topics:
        selected = topics[0]
        return {"selected_topic": json.dumps(selected, ensure_ascii=False)}
    return {"selected_topic": ""}

# æç¤º 3ï¼šç”Ÿæˆå¤§çº²
def generate_outline_node(state: AgentState):
    prompt = ChatPromptTemplate.from_template("""ä½ æ˜¯å†…å®¹ç»“æ„ä¸“å®¶ã€‚åŸºäºé€‰å®šçš„ä¸»é¢˜ï¼Œç”Ÿæˆè¯¦ç»†çš„å†…å®¹å¤§çº²ï¼š

ä¸»é¢˜ï¼š{selected_topic}

è¾“å‡ºæ ¼å¼ï¼šJSON æ ¼å¼ï¼ŒåŒ…å« outline æ•°ç»„ï¼Œæ¯ä¸ªå¤§çº²ç‚¹åŒ…å« section_title å’Œ key_pointsã€‚""")
    parser = JsonOutputParser()
    chain = prompt | llm | parser
    result = chain.invoke({"selected_topic": state["selected_topic"]})
    return {
        "outline": result.get("outline", []),
        "sections": [],
        "current_section_index": 0
    }

# æç¤º 4ï¼šåˆ†æ®µæ’°å†™ï¼ˆå¾ªç¯æ‰§è¡Œï¼‰
def write_section_node(state: AgentState):
    outline = state.get("outline", [])
    current_index = state.get("current_section_index", 0)
    previous_sections = state.get("sections", [])
    
    if current_index >= len(outline):
        # æ‰€æœ‰æ®µè½å·²å®Œæˆï¼Œåˆå¹¶ä¸ºå®Œæ•´è‰ç¨¿
        full_draft = "\n\n".join(previous_sections)
        return {"full_draft": full_draft}
    
    current_section = outline[current_index]
    previous_text = "\n\n".join(previous_sections) if previous_sections else "æ— "
    
    prompt = ChatPromptTemplate.from_template("""ä½ æ˜¯å†…å®¹æ’°å†™ä¸“å®¶ã€‚è¯·æ ¹æ®ä»¥ä¸‹å¤§çº²ç‚¹æ’°å†™å†…å®¹ï¼š

å¤§çº²ç‚¹ï¼š{section}
å‰æ–‡ä¸Šä¸‹æ–‡ï¼š{previous_sections}

è¦æ±‚ï¼š
- ä¸å‰æ–‡ä¿æŒè¿è´¯
- é£æ ¼ä¸€è‡´
- å†…å®¹ä¸°å¯Œ

è¾“å‡ºæ ¼å¼ï¼šMarkdown æ ¼å¼çš„å®Œæ•´æ®µè½ã€‚""")
    chain = prompt | llm | StrOutputParser()
    section_content = chain.invoke({
        "section": json.dumps(current_section, ensure_ascii=False),
        "previous_sections": previous_text
    })
    
    new_sections = previous_sections + [section_content]
    return {
        "sections": new_sections,
        "current_section_index": current_index + 1
    }

# åˆ¤æ–­æ˜¯å¦ç»§ç»­æ’°å†™
def should_continue_writing(state: AgentState) -> str:
    outline = state.get("outline", [])
    current_index = state.get("current_section_index", 0)
    if current_index < len(outline):
        return "write_section"
    else:
        return "optimize"

# æç¤º 5ï¼šæ•´ä½“ä¼˜åŒ–
def optimize_node(state: AgentState):
    prompt = ChatPromptTemplate.from_template("""ä½ æ˜¯å†…å®¹ä¼˜åŒ–ä¸“å®¶ã€‚è¯·å®¡é˜…å¹¶ä¼˜åŒ–ä»¥ä¸‹è‰ç¨¿ï¼š

å®Œæ•´è‰ç¨¿ï¼š{full_draft}

ä¼˜åŒ–æ–¹å‘ï¼š
- è¿è´¯æ€§å’Œæµç•…åº¦
- è¯­æ°”å’Œé£æ ¼ç»Ÿä¸€
- è¯­æ³•å’Œæ‹¼å†™
- é€»è¾‘ç»“æ„

è¾“å‡ºæ ¼å¼ï¼šä¼˜åŒ–åçš„å®Œæ•´ Markdown æ–‡æ¡£ã€‚""")
    chain = prompt | llm | StrOutputParser()
    result = chain.invoke({"full_draft": state["full_draft"]})
    return {"optimized_draft": result}

# æ„å»ºå›¾
workflow = StateGraph(AgentState)

# æ·»åŠ èŠ‚ç‚¹
workflow.add_node("generate_topics", generate_topics_node)
workflow.add_node("select_topic", select_topic_node)
workflow.add_node("generate_outline", generate_outline_node)
workflow.add_node("write_section", write_section_node)
workflow.add_node("optimize", optimize_node)

# å®šä¹‰è¿çº¿
workflow.set_entry_point("generate_topics")
workflow.add_edge("generate_topics", "select_topic")
workflow.add_edge("select_topic", "generate_outline")
workflow.add_edge("generate_outline", "write_section")
workflow.add_conditional_edges(
    "write_section",
    should_continue_writing,
    {
        "write_section": "write_section",
        "optimize": "optimize"
    }
)
workflow.add_edge("optimize", END)

# ç¼–è¯‘
app = workflow.compile()

# è¿è¡Œ
inputs = {
    "user_interests": "äººå·¥æ™ºèƒ½å’Œæœºå™¨å­¦ä¹ ",
    "content_type": "æŠ€æœ¯åšå®¢"
}
result = app.invoke(inputs)
print(result["optimized_draft"])
```

### 5. æœ‰çŠ¶æ€å¯¹è¯æ™ºèƒ½ä½“ ğŸ’­

**åœºæ™¯**ï¼šå¤šè½®å¯¹è¯ï¼Œéœ€è¦ç»´æŠ¤ä¸Šä¸‹æ–‡å’ŒçŠ¶æ€

**æç¤ºé“¾**ï¼š

1. å¤„ç†ç”¨æˆ·å‘è¨€ï¼Œè¯†åˆ«æ„å›¾å’Œå®ä½“
2. æ›´æ–°å¯¹è¯çŠ¶æ€ï¼ˆä¿å­˜å…³é”®ä¿¡æ¯ï¼‰
3. åŸºäºå½“å‰çŠ¶æ€ç”Ÿæˆå›å¤æˆ–è¯†åˆ«ä¸‹ä¸€æ­¥æ‰€éœ€ä¿¡æ¯
4. åç»­è½®æ¬¡é‡å¤ï¼Œæ¯æ¬¡æ–°å‘è¨€å¯åŠ¨é“¾å¼æµç¨‹ï¼Œåˆ©ç”¨ç´¯ç§¯çš„å¯¹è¯å†å²

**æµç¨‹æ„å»º**ï¼š

```python
from typing import TypedDict, List, Dict
from pydantic import BaseModel
from langgraph.graph import StateGraph, END
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser, JsonOutputParser
from langchain_openai import ChatOpenAI
import json

llm = ChatOpenAI(temperature=0)

class IntentAnalysis(BaseModel):
    intent: str
    entities: Dict[str, str]
    needs_clarification: bool

class AgentState(TypedDict):
    user_message: str
    conversation_history: List[str]
    intent_analysis: Dict
    current_state: Dict
    reply: str

# æç¤º 1ï¼šç†è§£ç”¨æˆ·æ„å›¾
def intent_node(state: AgentState):
    prompt = ChatPromptTemplate.from_template("""ä½ æ˜¯å¯¹è¯ç†è§£ä¸“å®¶ã€‚è¯·åˆ†æç”¨æˆ·çš„å‘è¨€ï¼Œè¯†åˆ«æ„å›¾å’Œå®ä½“ï¼š

ç”¨æˆ·å‘è¨€ï¼š{user_message}
å¯¹è¯å†å²ï¼š{conversation_history}

éœ€è¦è¯†åˆ«ï¼š
- ç”¨æˆ·æ„å›¾ï¼ˆè¯¢é—®/è¯·æ±‚/ç¡®è®¤ç­‰ï¼‰
- å…³é”®å®ä½“ï¼ˆäººå/åœ°ç‚¹/æ—¶é—´ç­‰ï¼‰
- éœ€è¦æ¾„æ¸…çš„ä¿¡æ¯

è¾“å‡ºæ ¼å¼ï¼šJSON æ ¼å¼ï¼ŒåŒ…å« intentã€entities å’Œ needs_clarificationã€‚""")
    parser = JsonOutputParser()
    chain = prompt | llm | parser
    history_text = "\n".join(state.get("conversation_history", []))
    result = chain.invoke({
        "user_message": state["user_message"],
        "conversation_history": history_text
    })
    return {"intent_analysis": result}

# æç¤º 2ï¼šæ›´æ–°å¯¹è¯çŠ¶æ€
def update_state_node(state: AgentState):
    intent_analysis = state.get("intent_analysis", {})
    current_state = state.get("current_state", {})
    
    # æ›´æ–°çŠ¶æ€ï¼šä¿å­˜å…³é”®å®ä½“å’Œæ„å›¾
    updated_state = {
        "last_intent": intent_analysis.get("intent", ""),
        "entities": intent_analysis.get("entities", {}),
        "needs_clarification": intent_analysis.get("needs_clarification", False),
        **current_state  # ä¿ç•™ä¹‹å‰çš„çŠ¶æ€
    }
    
    return {"current_state": updated_state}

# æç¤º 3ï¼šç”Ÿæˆå›å¤
def reply_node(state: AgentState):
    prompt = ChatPromptTemplate.from_template("""ä½ æ˜¯å¯¹è¯åŠ©æ‰‹ã€‚åŸºäºä»¥ä¸‹ä¿¡æ¯ç”Ÿæˆå›å¤ï¼š

ç”¨æˆ·æ„å›¾ï¼š{intent}
è¯†åˆ«å®ä½“ï¼š{entities}
å¯¹è¯å†å²ï¼š{conversation_history}
å½“å‰çŠ¶æ€ï¼š{current_state}

è¦æ±‚ï¼š
- ä¿æŒå¯¹è¯è¿è´¯æ€§
- åˆ©ç”¨å†å²ä¸Šä¸‹æ–‡
- å¦‚æœéœ€è¦æ›´å¤šä¿¡æ¯ï¼Œç¤¼è²Œåœ°è¯¢é—®

è¾“å‡ºæ ¼å¼ï¼šè‡ªç„¶æµç•…çš„å›å¤æ–‡æœ¬ã€‚""")
    chain = prompt | llm | StrOutputParser()
    history_text = "\n".join(state.get("conversation_history", []))
    result = chain.invoke({
        "intent": state["intent_analysis"].get("intent", ""),
        "entities": json.dumps(state["intent_analysis"].get("entities", {}), ensure_ascii=False),
        "conversation_history": history_text,
        "current_state": json.dumps(state["current_state"], ensure_ascii=False)
    })
    
    # æ›´æ–°å¯¹è¯å†å²
    new_history = state.get("conversation_history", []) + [
        f"ç”¨æˆ·ï¼š{state['user_message']}",
        f"åŠ©æ‰‹ï¼š{result}"
    ]
    
    return {"reply": result, "conversation_history": new_history}

# æ„å»ºå›¾
workflow = StateGraph(AgentState)

# æ·»åŠ èŠ‚ç‚¹
workflow.add_node("intent", intent_node)
workflow.add_node("update_state", update_state_node)
workflow.add_node("reply", reply_node)

# å®šä¹‰è¿çº¿
workflow.set_entry_point("intent")
workflow.add_edge("intent", "update_state")
workflow.add_edge("update_state", "reply")
workflow.add_edge("reply", END)

# ç¼–è¯‘
app = workflow.compile()

# è¿è¡Œï¼ˆç¬¬ä¸€è½®ï¼‰
inputs = {
    "user_message": "æˆ‘æƒ³äº†è§£äººå·¥æ™ºèƒ½",
    "conversation_history": [],
    "current_state": {}
}
result = app.invoke(inputs)
print(result["reply"])

# è¿è¡Œï¼ˆç¬¬äºŒè½®ï¼Œä½¿ç”¨ç¬¬ä¸€è½®çš„å†å²ï¼‰
inputs2 = {
    "user_message": "å®ƒæœ‰å“ªäº›åº”ç”¨ï¼Ÿ",
    "conversation_history": result["conversation_history"],
    "current_state": result["current_state"]
}
result2 = app.invoke(inputs2)
print(result2["reply"])
```

> **é‡ç‚¹**ï¼šæ¯è½®å¯¹è¯éƒ½å¯åŠ¨é“¾å¼æµç¨‹ï¼Œåˆ©ç”¨ç´¯ç§¯çš„å¯¹è¯å†å²ï¼ˆçŠ¶æ€ï¼‰ï¼Œä½¿ç³»ç»Ÿèƒ½è·¨å¤šè½®å¯¹è¯ä¿æŒä¸Šä¸‹æ–‡å’Œè¿è´¯æ€§ã€‚

### 6. ä»£ç ç”Ÿæˆä¸ä¼˜åŒ– ğŸ’»

**åœºæ™¯**ï¼šAI è¾…åŠ©å¼€å‘ï¼Œç”Ÿæˆå’Œä¼˜åŒ–ä»£ç 

**æç¤ºé“¾**ï¼š

1. ç†è§£ç”¨æˆ·ä»£ç éœ€æ±‚ï¼Œç”Ÿæˆä¼ªä»£ç æˆ–å¤§çº²
2. æ ¹æ®å¤§çº²æ’°å†™åˆç¨¿ä»£ç 
3. è¯†åˆ«ä»£ç æ½œåœ¨é”™è¯¯æˆ–æ”¹è¿›ç‚¹ï¼ˆå¯ç”¨é™æ€åˆ†æå·¥å…·æˆ–å†æ¬¡è°ƒç”¨ LLMï¼‰
4. æ ¹æ®é—®é¢˜é‡å†™æˆ–ä¼˜åŒ–ä»£ç 
5. è¡¥å……æ–‡æ¡£æˆ–æµ‹è¯•ç”¨ä¾‹

**æµç¨‹æ„å»º**ï¼š

```python
from typing import TypedDict, List, Dict
from pydantic import BaseModel
from langgraph.graph import StateGraph, END
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser, JsonOutputParser
from langchain_openai import ChatOpenAI
import json

llm = ChatOpenAI(temperature=0)

class RequirementAnalysis(BaseModel):
    description: str
    tech_stack: List[str]
    pseudocode: str

class CodeIssue(BaseModel):
    type: str
    severity: str
    suggestion: str

class AgentState(TypedDict):
    user_requirement: str
    language: str
    requirement_analysis: Dict
    generated_code: str
    review_issues: List[Dict]
    optimized_code: str
    documentation: str

# æç¤º 1ï¼šç†è§£éœ€æ±‚
def understand_node(state: AgentState):
    prompt = ChatPromptTemplate.from_template("""ä½ æ˜¯éœ€æ±‚åˆ†æä¸“å®¶ã€‚è¯·åˆ†æç”¨æˆ·çš„ä»£ç éœ€æ±‚ï¼š

ç”¨æˆ·éœ€æ±‚ï¼š{user_requirement}

è¾“å‡ºï¼š
- åŠŸèƒ½æè¿°
- æŠ€æœ¯æ ˆå»ºè®®
- ä¼ªä»£ç æˆ–ç®—æ³•æ€è·¯

è¾“å‡ºæ ¼å¼ï¼šJSON æ ¼å¼ï¼ŒåŒ…å« descriptionã€tech_stack å’Œ pseudocodeã€‚""")
    parser = JsonOutputParser()
    chain = prompt | llm | parser
    result = chain.invoke({"user_requirement": state["user_requirement"]})
    return {"requirement_analysis": result}

# æç¤º 2ï¼šç”Ÿæˆä»£ç 
def generate_code_node(state: AgentState):
    prompt = ChatPromptTemplate.from_template("""ä½ æ˜¯ä»£ç ç”Ÿæˆä¸“å®¶ã€‚æ ¹æ®ä»¥ä¸‹éœ€æ±‚ç”Ÿæˆä»£ç ï¼š

éœ€æ±‚åˆ†æï¼š{requirement_analysis}
ç¼–ç¨‹è¯­è¨€ï¼š{language}

è¦æ±‚ï¼š
- ä»£ç è§„èŒƒ
- åŒ…å«æ³¨é‡Š
- å¤„ç†è¾¹ç•Œæƒ…å†µ

è¾“å‡ºæ ¼å¼ï¼šå®Œæ•´çš„ä»£ç æ–‡ä»¶ï¼ŒåŒ…å«å¿…è¦çš„å¯¼å…¥å’Œå‡½æ•°ã€‚""")
    chain = prompt | llm | StrOutputParser()
    result = chain.invoke({
        "requirement_analysis": json.dumps(state["requirement_analysis"], ensure_ascii=False),
        "language": state["language"]
    })
    return {"generated_code": result}

# æç¤º 3ï¼šä»£ç å®¡æŸ¥
def review_code_node(state: AgentState):
    prompt = ChatPromptTemplate.from_template("""ä½ æ˜¯ä»£ç å®¡æŸ¥ä¸“å®¶ã€‚è¯·å®¡æŸ¥ä»¥ä¸‹ä»£ç ï¼Œè¯†åˆ«æ½œåœ¨é—®é¢˜ï¼š

ä»£ç ï¼š{generated_code}
åŸå§‹éœ€æ±‚ï¼š{user_requirement}

æ£€æŸ¥é¡¹ï¼š
- é€»è¾‘é”™è¯¯
- æ€§èƒ½é—®é¢˜
- ä»£ç é£æ ¼
- å®‰å…¨æ€§

è¾“å‡ºæ ¼å¼ï¼šJSON æ ¼å¼ï¼ŒåŒ…å« issues æ•°ç»„ï¼Œæ¯ä¸ªé—®é¢˜åŒ…å« typeã€severity å’Œ suggestionã€‚""")
    parser = JsonOutputParser()
    chain = prompt | llm | parser
    result = chain.invoke({
        "generated_code": state["generated_code"],
        "user_requirement": state["user_requirement"]
    })
    return {"review_issues": result.get("issues", [])}

# æç¤º 4ï¼šä¼˜åŒ–ä»£ç 
def optimize_code_node(state: AgentState):
    prompt = ChatPromptTemplate.from_template("""ä½ æ˜¯ä»£ç ä¼˜åŒ–ä¸“å®¶ã€‚è¯·æ ¹æ®å®¡æŸ¥ç»“æœä¼˜åŒ–ä»£ç ï¼š

åŸä»£ç ï¼š{generated_code}
å®¡æŸ¥é—®é¢˜ï¼š{review_issues}

è¾“å‡ºæ ¼å¼ï¼šä¼˜åŒ–åçš„å®Œæ•´ä»£ç ã€‚""")
    chain = prompt | llm | StrOutputParser()
    result = chain.invoke({
        "generated_code": state["generated_code"],
        "review_issues": json.dumps(state["review_issues"], ensure_ascii=False)
    })
    return {"optimized_code": result}

# æç¤º 5ï¼šç”Ÿæˆæ–‡æ¡£å’Œæµ‹è¯•
def document_node(state: AgentState):
    prompt = ChatPromptTemplate.from_template("""ä½ æ˜¯æ–‡æ¡£ä¸“å®¶ã€‚è¯·ä¸ºä»¥ä¸‹ä»£ç ç”Ÿæˆæ–‡æ¡£å’Œæµ‹è¯•ç”¨ä¾‹ï¼š

ä»£ç ï¼š{optimized_code}

è¦æ±‚ï¼š
- API æ–‡æ¡£
- ä½¿ç”¨ç¤ºä¾‹
- å•å…ƒæµ‹è¯•ç”¨ä¾‹

è¾“å‡ºæ ¼å¼ï¼šMarkdown æ ¼å¼çš„æ–‡æ¡£å’Œæµ‹è¯•ä»£ç ã€‚""")
    chain = prompt | llm | StrOutputParser()
    result = chain.invoke({"optimized_code": state["optimized_code"]})
    return {"documentation": result}

# æ„å»ºå›¾
workflow = StateGraph(AgentState)

# æ·»åŠ èŠ‚ç‚¹
workflow.add_node("understand", understand_node)
workflow.add_node("generate_code", generate_code_node)
workflow.add_node("review_code", review_code_node)
workflow.add_node("optimize_code", optimize_code_node)
workflow.add_node("document", document_node)

# å®šä¹‰è¿çº¿
workflow.set_entry_point("understand")
workflow.add_edge("understand", "generate_code")
workflow.add_edge("generate_code", "review_code")
workflow.add_edge("review_code", "optimize_code")
workflow.add_edge("optimize_code", "document")
workflow.add_edge("document", END)

# ç¼–è¯‘
app = workflow.compile()

# è¿è¡Œ
inputs = {
    "user_requirement": "å®ç°ä¸€ä¸ªå¿«é€Ÿæ’åºç®—æ³•",
    "language": "Python"
}
result = app.invoke(inputs)
print("ä¼˜åŒ–åçš„ä»£ç ï¼š")
print(result["optimized_code"])
print("\næ–‡æ¡£å’Œæµ‹è¯•ï¼š")
print(result["documentation"])
```

> **æ³¨æ„**ï¼šä»¥ä¸Šæ‰€æœ‰ç¤ºä¾‹éƒ½ä½¿ç”¨ LangGraph çš„ `StateGraph` æ¥æ„å»ºæç¤ºé“¾ï¼Œè¿™æ˜¯æ›´æ¨èçš„å®è·µæ–¹å¼ï¼Œå› ä¸ºå®ƒæä¾›äº†ï¼š
> - **çŠ¶æ€ç®¡ç†**ï¼šç»Ÿä¸€ç®¡ç†æ•´ä¸ªæµç¨‹çš„çŠ¶æ€
> - **çµæ´»æ§åˆ¶æµ**ï¼šæ”¯æŒæ¡ä»¶åˆ†æ”¯å’Œå¾ªç¯
> - **å¯æ‰©å±•æ€§**ï¼šæ˜“äºæ·»åŠ æ–°èŠ‚ç‚¹å’Œä¿®æ”¹æµç¨‹
> - **å¯è°ƒè¯•æ€§**ï¼šæ¯ä¸ªèŠ‚ç‚¹çš„è¾“å…¥è¾“å‡ºéƒ½å¯ä»¥æ¸…æ™°è¿½è¸ª

## æ€»ç»“

**æç¤ºé“¾é€šè¿‡å°†å¤æ‚é—®é¢˜æ‹†è§£ä¸ºä¸€ç³»åˆ—æ›´ç®€å•ã€æ˜“ç®¡ç†çš„å­ä»»åŠ¡ï¼Œä¸ºå¼•å¯¼å¤§è¯­è¨€æ¨¡å‹æä¾›äº†ç¨³å¥æ¡†æ¶**ã€‚åˆ†è€Œæ²»ä¹‹ç­–ç•¥è®©æ¨¡å‹æ¯æ¬¡ä¸“æ³¨äºå•ä¸€æ“ä½œï¼Œæ˜¾è‘—æå‡è¾“å‡ºçš„å¯é æ€§å’Œå¯æ§æ€§ã€‚

**å…³é”®è¦ç‚¹ï¼š**

- **æ˜ç¡®æ­¥éª¤ç›®æ ‡**ï¼šæ¯ä¸ªæç¤ºéƒ½æœ‰æ¸…æ™°ã€å•ä¸€çš„ç›®æ ‡
- **ç»“æ„åŒ–è¾“å‡º**ï¼šä½¿ç”¨ JSON/XML æ ¼å¼ç¡®ä¿æ•°æ®ä¼ é€’çš„å‡†ç¡®æ€§
- **è§’è‰²å®šä½**ï¼šä¸ºæ¯ä¸ªæ­¥éª¤åˆ†é…æ˜ç¡®çš„ä¸“ä¸šè§’è‰²
- **åˆç†è®¾è®¡æµç¨‹**ï¼šæ ¹æ®ä»»åŠ¡ç‰¹ç‚¹è®¾è®¡åˆé€‚çš„é“¾å¼æµç¨‹

> **ç»éªŒæ³•åˆ™**ï¼šå½“ä»»åŠ¡è¿‡äºå¤æ‚ã€åŒ…å«å¤šé˜¶æ®µå¤„ç†ã€éœ€åœ¨æ­¥éª¤é—´è°ƒç”¨å¤–éƒ¨å·¥å…·ï¼Œæˆ–éœ€æ„å»ºå¤šæ­¥æ¨ç†ã€çŠ¶æ€ç®¡ç†çš„æ™ºèƒ½ä½“æ—¶ï¼Œå»ºè®®é‡‡ç”¨æç¤ºé“¾æ¨¡å¼ã€‚
