# 使用和下载模型

## 安装环境

```shell
pip install transformers
```

## 加载在线模型（不推荐，很难成功）

1. 通过 `request` 请求
   - `API_URL` 为前缀 `https://api-inference.huggingface.co` + `model_name`

```python
import requests

API_URL = "https://api-inference.huggingface.co/models/uer/gpt2-chinese-cluecorpussmall"
API_TOKEN = "your_api_token_here"  # 替换为实际 API Token
headers = {"Authorization": f"Bearer {API_TOKEN}"}

# 发送文本生成请求
response = requests.post(API_URL, headers=headers, json={"inputs": "你好，我是一款语言模型，"})
print(response.json())
```

2. 使用 API

```python
from transformers import pipeline
# 替换为你的实际 API Token
API_TOKEN = "your_api_token_here"
# 使用 API Token
generator = pipeline("text-generation", model="gpt2", use_auth_token=API_TOKEN)
output = generator("The future of AI is", max_length=50)
print(output)
```

## 下载与使用模型

1. 公用下载模型方式（实际使用的**Model加载器**需要看对应模型的文档）

```python
from transformers import AutoModel, AutoTokenizer

#替换为你选择的模型名称
model_name = "xxx"
#指定模型保存路径
cache_dir = "./model_cache"

#下载并加载模型和分词器到指定文件夹
model = AutoModel.from_pretrained(model_name, cache_dir=cache_dir)
tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)
```

2. 使用本地模型
   - `model_dir`最好使用绝对路径，并且需要使用 `r` 保障字符串为原始字符串
   - `device`，可以指定模型运行的设备`cpu`或者`cuda`

```python
from transformers import AutoModel, AutoTokenizer

# 设置具体包含 config.json 的目录
model_dir = r"/Users/tal/Desktop/test/langchain-test/cache_model/models--bert-base-chinese/snapshots/8f23c25b06e129b6c986331a13d8d025a92cf0ea"  # 替换为实际路径

# 加载模型和分词器
model = AutoModel.from_pretrained(model_dir)
tokenizer = AutoTokenizer.from_pretrained(model_dir)

# 使用加载的模型和分词器创建分类任务的 pipeline
# text-classification 需要根据模型类型填写，device指定模型运行的设备“cpu”或者“cuda”
generator = pipeline("text-classification", model=model, tokenizer=tokenizer, device="cpu")

# 执行分类任务
output = generator("你好，我是一款语言模型")
print(output)
```

#### 文本生成模型的例子

下载模型

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

model_name = "uer/gpt2-chinese-cluecorpussmall" # 模型名称
cache_dir = "./cache_model" # 存放目录
# 下载模型
AutoModelForCausalLM.from_pretrained(model_name, cache_dir=cache_dir)
# 下载分词器
AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)
print(f"模型和分词器已下载到: {cache_dir}")
```

使用模型

```python
# 设置具体包含 config.json 的目录
model_dir = r"/Users/tal/Desktop/test/langchain-test/cache_model/models--uer--gpt2-chinese-cluecorpussmall/snapshots/c2c0249d8a2731f269414cc3b22dff021f8e07a3"

#加载模型和分词器
model = AutoModelForCausalLM.from_pretrained(model_dir)
tokenizer = AutoTokenizer.from_pretrained(model_dir)
# 使用加载的模型和分词器创建生成文本的 pipeline  device指定模型运行的设备“cpu”或者“cuda”
generator = pipeline("text-generation", model=model, tokenizer=tokenizer, device="cpu")
#文本生成
# output = generator("你好，我是一款语言模型",max_length=50,num_return_sequences=1)
output = generator(
    "你好，我是一款语言模型，",#生成文本的输入种子文本（prompt）。模型会根据这个初始文本，生成后续的文本
    max_length=50,#指定生成文本的最大长度。这里的 50 表示生成的文本最多包含 50 个标记（tokens）
    num_return_sequences=1,#参数指定返回多少个独立生成的文本序列。值为 1 表示只生成并返回一段文本。
    truncation=True,#该参数决定是否截断输入文本以适应模型的最大输入长度。如果 True，超出模型最大输入长度的部分将被截断；如果 False，模型可能无法处理过长的输入，可能会报错。
    temperature=0.7,#该参数控制生成文本的随机性。值越低，生成的文本越保守（倾向于选择概率较高的词）；值越高，生成的文本越多样（倾向于选择更多不同的词）。0.7 是一个较为常见的设置，既保留了部分随机性，又不至于太混乱。
    top_k=50,#该参数限制模型在每一步生成时仅从概率最高的 k 个词中选择下一个词。这里 top_k=50 表示模型在生成每个词时只考虑概率最高的前 50 个候选词，从而减少生成不太可能的词的概率。
    top_p=0.9,#该参数（又称为核采样）进一步限制模型生成时的词汇选择范围。它会选择一组累积概率达到 p 的词汇，模型只会从这个概率集合中采样。top_p=0.9 意味着模型会在可能性最强的 90% 的词中选择下一个词，进一步增加生成的质量。
    clean_up_tokenization_spaces=False#该参数控制生成的文本中是否清理分词时引入的空格。如果设置为 True，生成的文本会清除多余的空格；如果为 False，则保留原样。默认值即将改变为 False，因为它能更好地保留原始文本的格式。
)
print(output)
```

#### 文本分类模型的例子

下载模型

```python
from transformers import BertTokenizer, BertForSequenceClassification
from transformers import pipeline

model_name = "bert-base-chinese" # 模型名称
cache_dir = "./cache_model" # 存放目录
# 下载模型
model = BertForSequenceClassification.from_pretrained(model_name, cache_dir=cache_dir)
# 下载分词器
tokenizer = BertTokenizer.from_pretrained(model_name, cache_dir=cache_dir)
print(f"模型和分词器已下载到: {cache_dir}")
```

使用模型

```python
from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline

# 设置具体包含 config.json 的目录
model_dir = r"/Users/tal/Desktop/test/langchain-test/cache_model/models--bert-base-chinese/snapshots/8f23c25b06e129b6c986331a13d8d025a92cf0ea"  # 替换为实际路径

# 加载模型和分词器
model = AutoModelForSequenceClassification.from_pretrained(model_dir)
tokenizer = AutoTokenizer.from_pretrained(model_dir)

# 使用加载的模型和分词器创建分类任务的 pipeline
classifier = pipeline("text-classification", model=model, tokenizer=tokenizer, device="cpu")

# 执行分类任务
output = classifier("你好，我是一款语言模型")
print(output)
```

#### 问答模型的例子

下载模型

```python
from transformers import pipeline, AutoModelForQuestionAnswering, AutoTokenizer

model_name="uer/roberta-base-chinese-extractive-qa"
cache_dir="./cache_model"

AutoModelForQuestionAnswering.from_pretrained(model_name, cache_dir=cache_dir)
AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)
```

使用模型

```python
from transformers import pipeline, AutoModelForQuestionAnswering, AutoTokenizer

model_dir = r"/Users/tal/Desktop/test/langchain-test/cache_model/models--uer--roberta-base-chinese-extractive-qa/snapshots/9b02143727b9c4655d18b43a69fc39d5eb3ddd53"

model = AutoModelForQuestionAnswering.from_pretrained(model_dir)
tokenizer = AutoTokenizer.from_pretrained(model_dir)

# 本地加载中文RoBERTa问答模型
qa_pipeline = pipeline("question-answering", model=model, tokenizer=tokenizer, device="cpu")
# # 提问
result = qa_pipeline({
"question": "Hugging Face 是什么？",
"context": "Hugging Face 是一个自然语言处理平台。"
})
print(result)
```

